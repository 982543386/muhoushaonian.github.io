---
layout: post
title: Operations Research(三):非线性规划
description: 胡运权版《运筹学》学习与拓展
tag: OR
---

### 基本概念

1. 非线性规划的数学模型
   $$
   \begin{equation}
   \begin{cases}
   minf(X) \\
   h_i(X) = 0 (i = 1,2,...,m) \\
   g_j(X) \ge 0 (j = 1,2,...,l) \\
   \end{cases}
   \end{equation}
   $$
   其中$X=(x_1,x_2,x_3,...,x_n)^T$是n维空间中的点，目标函数$f(X)$和约束函数$h_i(X)、g_j(X)$为X的实函数

   **为什么会出现非线性规划**

   具有价格弹性的产品组合问题（给定产品在生产一个单位产品时的边际成本会随生产水平的不同而发生变化等） 、 运输成本存在总量折扣时的运输问题（边际运输成本）、存在风险的证券投资组合选择（目前，投资组合优化非线性规划在现代金融分析工作应用处于核心定位）

   **非线性规划问题的难点**
   
   最优解不一定在边界上，所以算法需要考虑可行域的所有解
   
   非线性规划中出现的另一个复杂情形是局部最大值不必为全局最大值，而非线性规划算法一般无法区分局部最大值和全局最大值，因此，清楚再什么条件下能够确保任何局部最大值均为可行域上的全局最大值就变得非常关键。
   
   **非线性规划问题的分类**
   
   对于非线性规划问题，主要可以分为无约束最优化、线性约束优化、二次规划、凸规划、可分规划和非凸规划等
   
   
   
   局部极小：设$f(X)$为定义在n维欧式空间$E_n$的某一区域R上的n元实函数: 对于$X^* \in R$, 如果存在某个$\epsilon > 0$, 使所有与$X^*$的距离小于$\epsilon$的$X\in R$, 都有$f(X) \ge f(X^*)$, 则称$X^*$为在R上的局部极小点, $f(X^*)$为局部极小值（去掉等号时为严格局部极小点和严格局部极小值）
   
   全局极小：设$f(X)$为定义在n维欧式空间$E_n$的某一区域R上的n元实函数，若存在$X^* \in R$, 对所有$X \in R$都有$f(X) \ge f(X^*)$, 则称$X^*$为$f(X)$在R上的全局极小点，$f(X^*)$为全局极小值
   
2. 多元函数极值点存在的条件

   我们在微积分中已经学习过二阶可微的一元函数f(x)极值点存在的条件，对于无约束多元函数，其极值点存在的必要条件和充分条件，与一元函数极值点的相应条件类似。

   a. 必要条件

   函数在该点的梯度值为0，即 $\nabla f(X^*) = 0$

   函数$f(X)$的梯度$\nabla f(X)$有两个十分重要的性质 

   1) 函数$f(X)$在某点的梯度必与过该点的等值面(等值线)正交

   2) 梯度向量的方向是函数值在该点处增加最快的方向，而负梯度方向则是函数值在该点处减少最快的方向

   b. 二次型

   正定 | 负定 | 不定的 | 半正定的 | 半负定的

   由线代相关知识，实二次型$X^TAX$为正定的充要条件，是它的矩阵A的左上角各阶主子式都大于零

   实二次型$X^TAX$为负定的充要条件，它的矩阵A的左上角顺序各阶主子式负、正相间

   c. 多元函数的泰勒公式

   设n元实函数$f(X)$在$X^{(0)}$的某一邻域内有连续二阶偏导数，则可写出它在$X^{(0)}$处的泰勒展开式
   $$
   f(X)=f(X^{(0)})+\nabla f(X^{(0)})^T(X-X^{(0)})+1/2(X-X^{(0)})^T\nabla^2f(\overline{X})(X-X^{(0)})
   $$
   

   d. 充分条件

![黑塞矩阵](D:\github_project\blog\muhoushaonian.github.io\images\post_image\黑塞矩阵.png)

3. 凹函数和凸函数

   设$f(X)$为定义在n维欧式空间$E_n$中某个凸集上的函数，若对任何实数$\alpha(0<\alpha<1)$以及凸集中的任意两点$X^{(1)}$和$X^{(2)}$, 恒有：
   $$
   f(\alpha X^{(1)} + (1-\alpha)X^{(2)}) \le \alpha f(X^{(1)}) + (1-\alpha)f(X^{(2)})
   $$
   则称$f(X)$为定义在$R_c$上的凸函数
   
   若对每一个$\alpha(0<\alpha<1)$和任意两点$X^{(1)} \neq X^{(2)} $,  恒有
   $$
   f(\alpha X^{(1)} + (1-\alpha)X^{(2)}) < \alpha f(X^{(1)}) + (1-\alpha)f(X^{(2)})
   $$
   则称$f(X)$为定义在$R_c$上的严格凸函数
   
   [将上述不等号方向即可得到凹函数和严格凹函数的定义]
   
4. 凸函数的性质和判定

   有限个凸函数的非负线性组合仍为凸函数

   a. 一阶判定

   设$R_c$为$E_n$上的开凸集，$f(X)$在$R_c$上可微，则$f(X)$为$R_c$上的凸函数的充要条件是:

   对任意不同两点$X^{(1)} \in R_c 和 X^{(2)} \in R_c$, 恒有：
   $$
   f(X^{(2)}) \ge f(X^{(1)}) + \nabla f(X^{(1)})^T(X^{(2)}-X^{(1)})
   $$
   b. 二阶条件

   设$R_c$为$E_n$上的开凸集，$f(X)$在$R_c$上二阶可微，则$f(X)$为$R_c$上的凸(凹)函数的充要条件是:

   对所有$X \in R_c$, 其黑塞矩阵半正定（半负定）

   【正定\负定 对应严格凹函数和严格凸函数】

   

   

5. 凸函数的极值

   现设$f(X)$是定义在凸集$R_c$上的可微凸函数，如果存在点$X^* \in R_c$, 使得对于所有的$X \in R_c$, 都有
   $$
   \nabla f(X^*)^T(X-X^*) \ge 0
   $$
   则$X^*$就是$f(X)$在$R_c$上的最小点



### 凸规划

$$
\begin{equation}\begin{cases}minf(X) \\g_j(X) \ge 0 (j = 1,2,...,l) \\\end{cases}\end{equation}
$$

若$f(X)$为凸函数，$g_i(X)(j=1,2,...,l)$全是凹函数，则称这种规划为凸规划

凸规划有着比较良好的性质：

a. 可行解集为凸集

b. 最优解集为凸集（假定最优解存在）

c. 任何局部最优解也是其全局最优解

d. 若目标函数为严格凸函数，且最优解存在，则其最优解必唯一



### 下降迭代算法

迭代法的基本思想：从最优点的某一个初始估计$X^{(0)}$出发，按照一定的规则，先找到一个比$X^{(0)}$更好的点$X^{(1)}$, 再找比$X^{(1)}$更好的点$X^{(2)}$，...，如此继续，就产生了一个解点的序列$\{X^{(k)}\}$,该点列的极限点或其中的某一点即为最优点

对于极小化问题，我们要求由选取的某一算法所产生的解的序列$\{X^{(k)}\}$, 其对应的目标函数值$f(X^{(k)})$应是逐步减小的，即要求：
$$
f(X^{(0)})>f(X^{(1)})>...>f(X^{(k)})
$$
*下降迭代算法的一般步骤*：

1. 选取某一初始点$X^{(0)}$, 令k = 0
2. 确定搜索方向：若已得出的某一迭代点$X^{(k)}$不是极小点。这时就从$X^{(k)}$出发确定一搜索方向$P^{(k)}$,沿这个方向应能找到使目标函数下降的点(对于约束极值问题，还要求这样的点是可行点)
3. 确定步长：通过选定步长因子$\lambda_k$得下一迭代点 $X^{(k+1)} = X^{(k)}+\lambda_k P^{(k)}$使得$f(X^{(k)}+\lambda_k P^{(k)})<f(X^{(k)})$
4. 检验新得到的点是否为要求的极小点或近似极小点，如满足要求，迭代停止。否则令k = k+1, 返回第二步重复进行迭代



*补充定理*：

设目标函数$f(X)$具有连续一阶偏导数，$X^{(k+1)}$按下述规则产生：
$$
\begin{equation}
\begin{cases}
f(X^{(k)}+\lambda P^{(k)}) = min _{\lambda}f(X^{(k)}+\lambda P^{(k)}) \\
X^{(k+1)} = X^{(k)} + \lambda_kP^{(k)}

\end{cases}
\end{equation}
$$
则有
$$
\nabla f(X^{(k+1)})^TP^{(k)} = 0
$$
文字表述为：在搜索方向上所得最优点处的梯度和该搜索方向正交



*判断迭代终止的条件*

1. 根据两次迭代结果的绝对误差（可以是点列中点的距离，也可以是函数值的差距）
   $$
   ||X^{(k+1)} - X^{k} || \le \varepsilon_1 \\
   |f(X^{(k+1)}) - f(X^k)| \le \varepsilon_2
   $$

2. 根据相继两次迭代结果的相对误差

3. 根据函数梯度的模足够小

   

   

   

   

### 一维搜索

一维搜索用于求解单变量的无约束极值问题，此处主要介绍Fibonacci法和0.618法



#### Fibonacci法







#### 0.618法





### 无约束极值问题

无约束极值问题的数学模型：

$min f(X) , X \in E$

梯度法（最速下降法）算法步骤：

a. 给定初始点$X^{(0)}$和允许误差, 令k=0

b. 计算$f(X^{(k)})$和$\nabla f(X^{(k)})$ , 若 $||\nabla f(X^{(k)})||^2 \le \varepsilon$, 停止迭代，得近似极小点和近似极小值，否则转下一步

c. 做一维搜索（$\lambda_k : min_\lambda f(X^{(k)}-\lambda \nabla f(X^{k}))$）, 并计算$X^{(k+1)} = X^{k}-\lambda_k \nabla f(X^{k})$, 然后令k = k+1

[将$f(X^{(k)}-\lambda \nabla f(X^{k})$在$X^k$作泰勒展开，并对$\lambda$求导，并令其等于0，即可得近似最佳步长的如下计算公式]

![image-20200515174115343](C:\Users\14573\AppData\Roaming\Typora\typora-user-images\image-20200515174115343.png)

最速下降法是一种理想的极小化方法（某点的负梯度方向，通常只是在该点附近才具有这种最速下降的性质），在实用中常将梯度法和其他方法联合应用，在前期使用梯度法，而在接近极小点时，可改用收敛较快的其他方法



牛顿法

基本思路: 通过二次函数逼近当前试解领域内的$f(x)$，然后以精确的方式使逼近函数最大化，以获得新的试解，从而启动下一次迭代（*采用目标函数二次逼近的思路已成为更普遍的非线性规划问题中所使用的诸多算法的一个关键特征*）

考虑一般n元实函数$f(X)$，假定它有连续二阶偏导数，$X^k$为其极小点的某一近似，在这个点附近取$f(X)$的二阶泰勒多项式逼近：
$$
f(X) \approx f(X^{(k)}) + \nabla f(X^{(k)}) \Delta X + \frac{1}{2} \Delta X^T \nabla^2f(X^{(k)}) \Delta X
$$
其中：$\Delta X = X-X^{(k)}$

该近似函数的极小点应满足一阶必要条件，即：
$$
\nabla f(X^{(k)}) + \nabla^2f(X^{(k)}) \Delta X=0
$$
当$\nabla^2f(X^{(k)})$的逆矩阵存在时，可得：、
$$
X = X^{(k)} - [\nabla^2f(X^{(k)})]^{(-1)}\nabla f(X^{(k)})
$$
上式为$f(X)$极小点的近似，为求得$f(X)$的极小点，可以按如下方式进行迭代：
$$
\begin{equation}
\begin{cases}
P^{(k)}=-[\nabla^2f(X^{(k)})]^{-1} \nabla f(X^{k}) \\
\lambda_k : min_\lambda f(X^{k} + \lambda P^{k}) \\
X^{(k+1)} = X^{k} + \lambda_k P^{k}
\end{cases}
\end{equation}
$$
该迭代方法即为所谓的阻尼牛顿法

NOTE: 当函数为正定二次函数$f(X)=\frac{1}{2} X^TAX + B^TX + c,此处A为 n*n对称正定阵$

此时可通过$X^* = X^{(0)} - A^{-1}\nabla f(X^{(0)})$一步收敛到极小点、



### 约束极值问题

#### 最优性条件（库恩-塔克条件）

1. 可行下降方向

   a. 起作用约束

   ​    假定$X^{(0)}$是问题的一个可行解，它满足所有的约束条件；对某一个约束条件$g_i(X) \ge 0$来说，$X^{(0)}$满足它有两种情况：一种情况$g_j(X^{(0)}) > 0$，这时为不起作用约束；另一种情况$g_j(X^{(0)}) = 0$，这时$X^{(0)}$点处于由这个约束条件形成的可行域边界上，为起作用约束.

   b. 可行方向

   ​	设$X^{(0)}$为任一可行点，对某一方向$P$来说，若存在实数$\lambda_0 > 0$，使得对任意的$\lambda \in [0,\lambda_0]$，均有下式成立：$X^{(0)} + \lambda P \in R$， 就称方向P为$X^{(0)}$的一个可行方向。[另外通过约束条件可将得可行方向的判定条件，只要方向$P$满足 $\nabla g_j(X^{(0)})^T P > 0$], 则方向P为$X^{(0)}$的可行方向

   c. 下降方向

   设$X^{(0)}$为任一可行点，对某一方向$P$来说，若存在实数$\lambda^{'}_0 > 0$，使得对任意的$\lambda \in [0,\lambda^{'}_0]$，均有下式成立：$f(X^{(0)} + \lambda P) < f(X^{(0)})$，就称方向$P$为$X^{(0)}$点的一个下降方向。[由泰勒展开式可得下降方向的判定条件：$\nabla f(X^{(0)})^T P < 0$]

   d. 可行下降方向

   若$X^{(0)}$点的某一方向$P$，即是该点的可行方向，又是该点的下降方向，就称它为这个点的可行下降方向（显然在极小点处不存在可行下降方向）

2. 库恩-塔克（Kuhn-Tucker）条件

   Gordan引理    Fritz John定理

   库恩-塔克条件是确定某点为最优点的必要条件，只要是最优点，且此处起作用约束的梯度线性无关，一般来说它只是必要条件。但对于凸规划问题而言，它既是充分条件也是必要条件。

   - [ ] 补充详细公式



#### 制约函数法

制约函数法，即通过构造某种制约函数并将其加到非线性规划的目标函数上，从而将原来的约束极值问题，转化为无约束极值问题来求解，此处主要介绍罚函数法（外点法）和障碍函数法（内点法）

*罚函数法（外点法）*

首先来直观的理解外点法，我们把目标函数$f(X)$看成一种价格，约束条件看成一种“规定”，每个人可以在规定范围内采购商品，当人们买规定外的商品时，我们处以相应的罚款，每个人的终极目标就是使*采购实际金额+罚款*总额最小.

罚函数的构建：

对于即包含等式约束又包含不等式约束的一般非线性规划问题，其罚函数为：
$$
P(X,M) = F(X)+M \sum_{i=1}^{m}[h_i(X)]^2+M \sum_{j=1}^{l}[g_j(X)]^2u_j(g_j(X)) \\
Note: u_j(g_j(X))为阶跃函数
$$
罚函数法的迭代步骤：

1. 取第一个罚因子$M_i>0$，允许误差$\varepsilon > 0$，并令$k = 1$

2. 求下述无约束极值问题的最优解： $min P(X,M_k)$

3. 若存在某一个$j(1\le j\le l)$有$-g_j(X^{k}) > \varepsilon$或存在某一个$i(1 \le i \le m)$，有$|h_i(X^{k})|>\varepsilon$，则取$M_{k+1}>M_k$，并令$k=2$，转向第二步重新开始迭代过程；否则迭代终止，得到所有点

   [罚函数法得到的实际是与M取值序列对应的序列点集，另外迭代过程往往是在可行域外进行的，因而中间结果不能直接作为近似解使用（最后得到的解可能不是可行解）]



*障碍函数法(内点法)*

障碍函数法与罚函数法不同，它要求迭代过程始终在可行域内部进行（相当于在边界上设置了一道屏障，使得迭代过程始终在可行域内部进行）

障碍函数的构建：

当$X$从可行域内部趋于其边界时，至少有某一个约束函数$g_j(X)$趋于零，倒数函数$\sum_{j=1}^{l}\frac{1}{g_j(X)}$和对数函数$- \sum_{j=1}^{l}lg(g_j(X))$都将无限增大. 则我们可以将原约束优化问题转化为无约束极值优化问题：



![内点法](D:\github_project\blog\muhoushaonian.github.io\images\post_image\内点法.png)

内点法的迭代步骤：

1. 取第一个障碍因子$r_1>0$，允许误差$\varepsilon > 0$，并令$k = 1$
2. 构造障碍函数，障碍项可采用倒数函数，也可采用对数函数
3. 对障碍函数进行无约束极小化
4. 检查是否满足收敛准则，满足则停止迭代，否则返回上一步继续进行迭代



